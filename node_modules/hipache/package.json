{
  "name": "hipache",
  "version": "0.2.4",
  "description": "Complete high-scaled reverse-proxy solution",
  "keywords": [
    "reverse",
    "proxy",
    "http",
    "http-proxy"
  ],
  "main": "./bin/hipache",
  "bin": {
    "hipache": "./bin/hipache"
  },
  "directories": {
    "lib": "./lib"
  },
  "dependencies": {
    "read-installed": "0.2.2",
    "http-proxy": "git://github.com/samalba/node-http-proxy",
    "redis": "0.8.x",
    "lru-cache": "2.2.x",
    "optimist": "0.3.x"
  },
  "scripts": {
    "start": "node bin/hipache",
    "test": "nodeunit test/test_*.js",
    "jshint": "jshint --config jshint.json -- bin/hipache lib/*.js"
  },
  "engines": {
    "node": ">= 0.8.0"
  },
  "engineStrict": true,
  "author": {
    "name": "Samuel Alba",
    "email": "sam.alba@gmail.com",
    "url": "http://www.dotcloud.com/"
  },
  "maintainer": [
    "Sam Alba <sam.alba@gmail.com>"
  ],
  "repository": {
    "type": "git",
    "url": "http://github.com/dotcloud/hipache.git"
  },
  "contributors": [
    {
      "name": "Samuel Alba",
      "email": "sam.alba@gmail.com"
    }
  ],
  "readme": "Hipache: a distributed HTTP and websocket proxy\n===============================================\n\nWhat is it?\n-----------\n\nHipache is a distributed proxy designed to route high volumes of http and\nwebsocket traffic to unusually large numbers of virtual hosts, in a highly\ndynamic topology where backends are added and removed several times per second.\nIt is particularly well-suited for PaaS (platform-as-a-service) and other\nenvironments that are both business-critical and multi-tenant.\n\nHipache was originally developed at [dotCloud](http://www.dotcloud.com), a\npopular platform-as-a-service, to replace its first-generation routing layer\nbased on a heavily instrumented nginx deployment. It currently serves\nproduction traffic for tens of thousands of applications hosted on dotCloud.\nHipache is based on the node-http-proxy library.\n\n\nRun it!\n-------\n\n### 1. Install it\n\nFrom the shell:\n\n    $ npm install hipache -g\n\n*The '-g' option will make the 'hipache' bin-script available system-wide (usually linked from '/usr/local/bin')*\n\n\n### 2. Configuring the server (config.json)\n\ndotCloud proxy2 uses a Redis server to manage its configuration (and to share\nits state across the multiple workers). You can use the Redis server to change\nits configuration while it's running or simply check the health state of a\nbackend.\n\n    {\n        \"server\": {\n            \"accessLog\": \"/var/log/hipache_access.log\",\n            \"port\": 80,\n            \"workers\": 5,\n            \"maxSockets\": 100,\n            \"deadBackendTTL\": 30,\n            \"https\": {\n                \"port\": 443,\n                \"key\": \"/etc/ssl/ssl.key\",\n                \"cert\": \"/etc/ssl/ssl.crt\"\n            }\n        },\n        \"redisHost\": \"127.0.0.1\",\n\t\"redisPort\": 6379\n    }\n\n* __server.accessLog__: location of the Access logs, the format is the same as\nnginx\n* __server.port__: Port to listen to (HTTP)\n* __server.workers__: Number of workers to be spawned (specify at least 1, the\nmaster process does not serve any request)\n* __server.maxSockets__: The maximum number of sockets which can be opened on\neach backend (per worker)\n* __server.deadBackendTTL__: The number of seconds a backend is flagged as\n`dead' before retrying to proxy another request to it\n* __server.https__: SSL configuration (omit this section to disable HTTPS)\n* __redisHost__ and __redisPort__: Redis configuration (you can omit those\nparameters to use the local redis on the default port)\n\n\n### 3. Spawn the server\n\nFrom the shell:\n\n    $ hipache\n\nOr if you use the port 80:\n\n    $ sudo hipache\n\nOr by specifying your configuration file:\n\n    $ hipache --config config.json\n\n__Managing multiple configuration files:__\n\nThe default configuration file is `config.json`. It's possible to have\ndifferent configuration files named `config_<suffix>.json`. The suffix is got\nfrom an environment variable called `SETTINGS_FLAVOR`.\n\nFor instance, here is how to spawn the server with the `config_test.json`\nconfiguration file in order to run the tests.\n\n    $ SETTINGS_FLAVOR=test hipache\n    \n\n### 4. Configuring a vhost (redis)\n\nAll the configuration is managed through Redis. This makes it possible to\nupdate the configuration dynamically and gracefully while the server is\nrunning.\n\nIt also makes it simple to write configuration adapters. It would be trivial\nto load a plain text configuration file into Redis (and update it at runtime).\n\nDifferent configuration adapters will follow, but for the moment you have to\nprovision the Redis manually.\n\nLet's take an example, I want to proxify requests to 2 backends for the\nhostname www.dotcloud.com. The 2 backends IP are 192.168.0.42 and 192.168.0.43\nand they serve the HTTP traffic on the port 80.\n\n`redis-cli` is the standard client tool to talk to Redis from the terminal.\n\nHere are the steps I will follow:\n\n1. __Create__ the frontend and associate an identifier\n\n        $ redis-cli rpush frontend:www.dotcloud.com mywebsite\n        (integer) 1\n\nThe frontend identifer is `mywebsite`, it could be anything.\n\n2. __Associate__ the 2 backends\n\n        $ redis-cli rpush frontend:www.dotcloud.com http://192.168.0.42:80\n        (integer) 2\n        $ redis-cli rpush frontend:www.dotcloud.com http://192.168.0.43:80\n        (integer) 3\n\n3. __Review__ the configuration\n\n        $ redis-cli lrange frontend:www.dotcloud.com 0 -1\n        1) \"mywebsite\"\n        2) \"http://192.168.0.42:80\"\n        3) \"http://192.168.0.43:80\"\n\nWhile the server is running, any of these steps can be re-run without messing\nup with the traffic.\n\n\nFeatures\n--------\n\n### Load-balancing across multiple backends\n\nAs seen in the example above, multiple backends can be attached to a frontend.\n\nAll requests coming to the frontend are load-balanced across all healthy\nbackends.\n\nThe backend to use for a specific request is determined at random. Subsequent\nrequests coming from the same client won't necessarily be routed to the same\nbackend (since backend selection is purely random).\n\n### Dead backend detection\n\nIf a backend stops responding, it will be flagged as dead for a\nconfigurable amount of time. The dead backend will be temporarily removed from\nthe load-balancing rotation.\n\n### Multi-process architecture\n\nTo optimize response times and make use of all your available cores, Hipache\nuses the cluster module (included in NodeJS), and spreads the load across\nmultiple NodeJS processes. A master process is in charge of spawning workers\nand monitoring them. When a worker dies, the master spawns a new one.\n\n### Memory monitoring\n\nThe memory footprint of Hipache tends to grow slowly over time, indicating\na probable memory leak. A close examination did not turn up any memory leak\nin Hipache's code itself; but it doesn't prove that there is none. Also,\nwe did not investigate (yet) thoroughly the code of Hipache's external\ndependencies, so the leaks could be creeping there.\n\nWhile we profile Hipache's memory to further reduce its footprint, we\nimplemented a memory monitoring system to make sure that memory use doesn't\ngo out of bounds. Each worker monitors its memory usage. If it crosses\na given threshold, the worker stops accepting new connections, it lets\nthe current requests complete cleanly, and it stops itself; it is then\nreplaced my a new copy by the master process.\n\n### Dynamic configuration\n\nYou can alter the configuration stored in Redis at any time. There is no\nneed to restart Hipache, or to signal it that the configuration has changed:\nHipache will re-query Redis at each request. Worried about performance?\nWe were, too! And we found out that accessing a local Redis is helluva fast.\nSo fast, that it didn't increase measurably the HTTP request latency!\n\n### WebSocket\n\nHipache supports the WebSocket protocol. It doesn't do any fancy handling\nfor the WebSocket protocol; it relies entirely on the support in NodeJS\nand node-http-proxy.\n\n### SSL\n\nIf provided with a SSL private key and certificate, Hipache will support SSL\nconnections, for \"regular\" requests as well as WebSocket upgrades.\n\n### Custom HTML error pages\n\nWhen something wrong happens (e.g., a backend times out), or when a request\nfor an undefined virtual host comes in, Hipache will display an error page.\nThose error pages can be customized.\n\n### Wildcard domains support\n\nWhen adding virtual hosts in Hipache configuration, you can specify wildcards.\nE.g., instead (or in addition to) www.example.tld, you can insert\n*.example.tld. Hipache will look for an exact match first, and then for a\nwildcard one.\n\nNote that the current implementation only tries to match wildcards against the\nlast two labels of the requested virtual host. What does that mean? If you\nissue a request for some.thing.example.tld, Hipache will look for *.example.tld\nin the configuration, but not for *.thing.example.tld. If you want to serve\nrequests for *.thing.example.tld, you will have to setup a wildcard for\n*.example.tld. It means that you cannot (yet) send requests for\n*.thing.example.tld and *.stuff.example.tld to different backends.\n\n### Active Health-Check\n\nEven though Hipache support passive health checks, it's also possible to run\nactive health checks. This mechanism requires to run an external program,\nyou can find it on the [hipache-hchecker project page.](https://github.com/samalba/hipache-hchecker)\n\n\nFuture improvements\n-------------------\n\n[Read the TODO page](https://github.com/dotcloud/hipache/blob/master/TODO.md)\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/dotcloud/hipache/issues"
  },
  "_id": "hipache@0.2.4",
  "dist": {
    "shasum": "56f34ec7a5cee4311efb8b4acd90b12e5e4f3968"
  },
  "_from": "hipache@",
  "_resolved": "https://registry.npmjs.org/hipache/-/hipache-0.2.4.tgz"
}
